**WIP ---- WIP ---- WIP ---- WIP**

# Guidelines

FuseML follow a combination of design and architecture patterns and best practices:
* [the 12 factor app](https://12factor.net/)
* clean architecture
* domain driven design



# Domain Entities

<img src="./img/fuseml-domain.svg">

## Artifacts

Artifacts can be generally described as immutable objects at rest that are either generated by or accepted as input by AI/ML workloads orchestrated by FuseML. They are internally represented by an artifact descriptor which among other things includes an external reference to the service location where the actual artifact contents are stored. Some example of artifacts are: ML models (can be just model definitions or full blown pre-trained models), container images built for various ML operations (data transformation, training, inference serving etc.), ML code components or packages and datasets.

Artifacts have a unique identity and their contents are static. The immutability aspect is vital for features like gitops operations, end-to-end lineage and reproducibility. Artifacts may additionally be referenced using a variety of identification schemes (versioning, tagging, labeling etc.) that don't need to be static (i.e. may change during the lifetime of an artifact). This abstraction should also extend past code and container images and cover the full range of objects involved in AI/ML operations, including but not limited to models and datasets.

Artifacts play another important role in constructing MLOps pipelines: they facilitate composability and reusability. Artifacts act as connection points that enable building a pipeline out of reusable components. Two or more pipeline components can be coupled together to form higher level operational concepts by listing in their definition the type of artifacts that they consume and generate. When artifacts are well described, building complex functional pipelines out of components becomes a simple exercise of matching inputs to outputs.

FuseML must be able to track all artifacts used or generated by AI/ML workloads and define other conventions around artifacts that are needed to implement additional features important from an MLOps workflow perspective. For example:
* searching for artifacts using compatibility filters (e.g. ML models compatible with TensorFlow, predictors compatible with Seldon Core)
* navigating the history of artifacts generated by subsequent runs of the pipeline or runnable that generated them or used them as input

From a design standpoint, artifacts are not operated on directly. All operations applicable to artifacts (e.g. registering, updating, deleting, searching artifacts) are implemented through [Artifact Stores](#ArtifactStores).

Artifacts are further classified into:
* codeset artifacts - the unit of organization for source code
* runnable artifacts - the unit of execution in an MLOps pipeline
* machine learning models
* datasets

### Codeset artifacts

A codeset can be described as a collection of files: code files, scripts, configuration files, generally all the sources needed to implement, build and execute the machine learning logic that goes into the automated ML operations orchestrated by FuseML. It often happens with sample AI/ML applications that datasets and even ML models are grouped and stored alongside regular code and configuration files, but this practice is not advisable, because datasets and ML models represent different categories of AI/ML artifacts, with their own specific lifecycle and operations.

A codeset artifact is an immutable snaphot of a codeset. Even though this is a domain element that should be decoupled from back-end technologies, FuseML makes the assumption that source version control services are used to manage codeset artifacts and therefore relies on representative semantic concepts. From this standpoint, a codeset artifact is uniquely represented by a commit in a git repository. Higher level abstractions such as branches and organizations may be used to implement other FuseML concepts and operations.

### Runnable artifacts

FuseML runnables are the composable, reusable and shareable building blocks used for ML workflow operations. A runnable is represented by an OCI container image paired with one or more FuseML specific descriptors that describe how the container image can be used as a component in the MLOps workflow. Runnables are also one of the most important extensibility mechanisms employed by FuseML to integrate with 3rd party AI/ML tools in a loosely coupled manner.

The complete format of the FuseML runnable descriptor is outside the scope of this document, but some suggestions of fields it should include are given here:
* runnable kind - this field can be used to differentiate between several sub-classes of runnables, each with its own specifics regarding how they are organized and operated on by FuseML or its extensions
* entry point (command, binary or script to execute)
* list of named and typed inputs and outputs. Some possible options are:
  * artifacts or artifact references (e.g. datasets, models, other runnables), optionally accompanied by additional specifiers describing the artifact type, provenance, version etc. Specifiers can be used for example to further filter the artifacts that match the inputs compatible with the runnable, make suggestions about which runnables can be coupled, detect incorrect pipeline setups etc.
  * for input artifacts in particular, the descriptor might include additional details describing statically what constitutes a change (e.g. which files to watch for in an input codeset artifact, which fields to watch for in an input data artifact and which metrics to watch for in an input model artifact)
  * parameters passed by value (e.g. strings, integers, booleans). These can be modeled to take several forms at runtime: 
    * environment variables
    * command line arguments
    * config maps
* resource requirements (ram, cpu, gpu, storage)
* capability requirements (e.g. cpu type, gpu type)

FuseML and its extensions must automatically handle all aspects needed to provide the artifacts used as input by the runnable's container, as well as collect and register the artifacts generated by the container as outputs, in order to isolate the builder's code from the FuseML specific aspects of storing and versioning the artifacts (e.g. clone or copy the git repo/commit, model or dataset into a volume where the container has access).

Examples of kinds of runnable artifacts that could be explicitly modeled as core FuseML concepts are covered in the next sections:
* builders - used to build other runnables out of codeset artifacts 
* trainers - used to train a machine learning model
* predictors - used to run predictions on a pre-trained model

More kinds of runnables will be modeled as they are identified as common patterns that need to be represented as core FuseML concepts, as we integrate additional 3rd party AI/ML tools with FuseML, for example:
* notebook servers - used as development environments
* data transformers - used for data ETL and paired with a feature store
* model validators - used to validate a pre-trained model against test or new data 
* model explainers

#### Builders

Builders are runnables that build other runnables out of codeset artifacts. Builders can themselves be built out of code using other builders and so on, in a recurring manner. FuseML should provide some default builder runnables and conventions that can in turn be used to create other builders specifically designed for 3rd party tools.

What is specific about these runnables:
  * inputs include one (or more) codeset artifact(s)
    * sources to be built into runnables
    * scripts used to build the runnables
    * configuration files controlling how to build the runnables
  * outputs include one or more runnable artifacts
  * other types of artifacts should be allowed as inputs and outputs. For example:
    * a builder might take in an codeset artifact that represents machine learning code bundled together with datasets and models. Aside from a trainer artifact, this builder may create additional dataset artifacts and model artifacts corresponding to those bundled models and datasets.
    * a builder might take in another runnable artifact e.g. if it needs to extend its logic
  * FuseML must facilitate the creation and registration of runnable artifacts by providing one or more of the following
    * CLI
    * SDK libraries
    * container images bundling together the CLI/SDK with other common tools required to define runnables and their software requirements. These container images can be used directly or as base images for tool specific images.
    * automation for building container images: the builder should not be concerned with _building_ container images. It suffices to provide Dockerfiles describing these images and let FuseML take care of all aspects of building, storing, tagging etc. 

Examples of builder artifacts:
1. MLFlow builder - builds trainer and predictor runnables out of an MLFlow project
   * ID: MLFlow project builder
   * inputs:
     * a codeset artifact of type "MLFlow project", with a change filter configured to trigger only when the `conda.yaml` or `MLProject` files change
     * (implicit) a codeset artifact of type "MLFlow builder" - contains the scripts required to build MLFlow runnable artifacts
   * outputs:
     * a trainer artifact of type "MLFLow" (described below)
     * a predictor artifact of type "MLFLow" (described below)
   * OCI container:
     * stock image (or derivation of) provided by FuseML for all builder runnable artifacts, supplying base software requirements (FuseML CLI or SDK library)
   * entry point: the custom builder logic provided with the "MLFlow builder" codeset artifact that leverages the `MLProject` and `conda.yaml` files bundled within the MLFlow codeset artifact:
     * determines if changes from the previous input artifact versions warrant a rebuild of trainer/predictor artifacts (e.g. did the `conda.yaml` file change ? or the builder logic itself ? or was the builder triggered manually or scheduled to run ?) 
     * creates a Dockerfile for the container image used in common by trainer and predictor (installs mlflow and all software requirements specified in `conda.yaml`)
     * uses FuseML CLI/SDK to register the Dockerfile and the descriptors for the two runnables with FuseML
       * the runnables will be tied to the same input codeset artifact of type "MLFlow project" as one used to run the builder
       * the parameters for the training runnable are inferred from the `MLProject` file

   Usage example: whenever a new MLFlow project version is registered as a codeset artifact, the builder is triggered and registers new versions of the "MLFlow" trainer and predictor runnables. FuseML continues the process by rebuilding the container image before recording the new runnables in its database and triggering other workflows that have been affected.

#### Trainers

Trainers are runnables that can be used to train a machine learning model. 

What is specific about these runnables:
  * inputs include:
    * one or more dataset artifact(s) (or zero, if the dataset is included in the container image, in another artifact or obtained through other means that don't involve FuseML)
    * training parameters (can be used to do hyperparameter tuning) 
  * outputs include:
    * one or more model artifacts
    * model metrics
  * other types of artifacts should be allowed as inputs and outputs. For example:
    * a trainer might take in codeset artifacts, for one of these reasons:
      * that's where the dataset or model definition are bundled
      * because the codeset artifact contains other files required for training (e.g. parameters)
      * the training code itself is bundled there (i.e. code doesn't need to be built or packaged, python for example)
    * the trainer may generate one or more datasets (e.g. the processed dataset used for training, the one used for validation) to be used for later reference (e.g. to detect when drifts occur)
  * FuseML must facilitate the creation and registration of model artifacts by providing one or more of the following
    * CLI
    * SDK libraries (e.g. python libraries extending popular machine learning libraries and providing means of storing trained models)

Examples of trainer artifacts:
1. MLFlow trainer - trains a model out of a specific MLFlow project
   * ID: MLFlow trainer for project X
   * inputs:
     * the same codeset artifact of type "MLFlow project" that was used to build this trainer
     * environment variables pointing to the MLFlow instance where experiments are logged (to be determined if and how these are provided)
     * (optional) URL to data
     * training parameters (extracted from the `MLProject` file)
   * outputs:
     * a model of type "MLFLow" (or a model of type "Y" where Y is the ML library used to train and save the model - e.g. sklearn, pytorch, tensorflow)
   * OCI container:
     * custom container image build by the "MLFlow builder" runnable and including all software requirements characteristic to the project
   * entry point: script wrapping `mlflow run`:
     * runs the project (assumes it does training among other things)
     * detects the saved model from the output
     * uses FuseML CLI/SDK to register the model with FuseML
     * alternatively, the MLFlow project code files themselves use the SDK to register the model with FuseML, or FuseML is already integrated with MLFlow by other means and can detect when a model is registered

   Usage example: whenever a new version of the target MLFlow project codeset is registered or when a new trainer version is built, the trainer is triggered to train and register a new version of the model. The trainer can also be triggered to run manually, with different training parameter or with different data.

### Predictors

To be detailed.

## Pipelines

A pipeline is a collection of runnables coupled together into a DAG (direct acyclic graph) workflow by matching together their inputs and outputs. Running a pipeline results in spawning a number of jobs and services that The definition of a pipeline includes the conditions that need to be met for the pipeline run to occur.

Pipelines are the primary consumers of artifacts. If pipeline components can consume inputs from and save outputs to a centralized repository, they no longer have to be tightly coupled together into atomic workflows that depend on eachother at runtime in order to be functional. Pipeline components become standalone functions that can be executed individually. This in turn supports a range of features otherwise not easily achievable:
* running a pipeline step-by-step in a controlled manner (e.g. for debugging purposes)
* dynamically swapping components while a pipeline is running
* resuming a failed pipeline from where it failed instead of running the entire pipeline all over agaoin
* supporting end-to-end MLOps workflows that are not fully automated and need to be split into segments that require some human interaction or external events to continue (e.g. advancing a trained model from development to staging to production)
* running pipeline segments in different infrastructure domains
* scheduling pipeline segments to run at different times to avoid resource contention

Implementing generic pipeline operations without getting locked into a specific 3rd party tool or type of infrastructure requires manipulating artifacts in a generic way, without exposing the core services to implementation specific details such as accessing persistent storage, indexing, searching and caching. This is especially important with 3rd party AI/ML tools that manage their own form of repository for artifacts.

FuseML does not make it mandatory that all steps in a pipeline register their output artifacts with an artifact store through FuseML. In some cases, it is advisable to keep artifacts anonymous, when for example artifacts are not relevant, or when it is not efficient to sync artifacts with a central store due to size concerns. Artifacts can be passed between pipeline components by using local, temporary storage. Lineage and reproducibility features will not be available, but only isolated to the pipeline steps that don't follow this convention. Interoperability (different pipeline steps implemented by different tools) is still achievable with anonymous artifacts, if they are well defined, even if they are not stored, but this still creates additional dependencies between pipeline steps regarding inter-operability and local artifact storage.

NOTES to be expanded: 
* the serving service instance is represented as the output of a pipeline component, _not_ as a component of a pipeline. In that sense, it resembles more an artifact than it is a pipeline component.
* maybe the full lifecycle of the serving services is optional, or even completely out of scope for FuseML

# Repositories

<img src="./img/fuseml-design.svg">

## Artifact Store

Artifact stores are FuseML components that group and manage together artifacts of the same type and implement the generic interface defined by FuseML that describes the artifact operations required by FuseML core services. Artifact stores are one of the extensibility mechanisms supported by FuseML. All artifact stores with the exception of those that are built-in (e.g. the gitea application store and the OCI runnable store) should be implemented as independent micro-services - artifact store agents - to enable runtime extensibility. The FuseML design should also allow for multiple artifact stores to be implemented for the same kind of artifact. 

To simplify lifecycle management for artifact store agents, the same mechanisms implemented by FuseML and used for MLOLps could be reused for this extensibility purpose:
* a special type of runnable, similar to a predictor, is used to run artifact store agents, exposing a REST or gRPC API that is consumed by the FuseML core services
* FuseML provides all the items needed to simplify defining and implementing new agent runnables: base classes, wrappers, base container images etc.

Another design assumption that FuseML makes about artifact management in general is that the two parts that make an artifact - the descriptor and the contents - are not necessarily co-located. This assumption is based on several observations:
1. the requirements regarding persistent storage of and, more importantly, access to artifact descriptors are significantly different than those for artifact contents. FuseML core services should rarely need to access the artifact contents, but rely heavily on the information stored in artifact descriptors, whereas for actual AI/ML workloads it's the artifact contents that are important. Solutions oriented predominantly towards file storage (e.g. git servers, S3 storage) may be suitable for artifact contents, which are usually large and/or opaque objects, but they're inadequate for artifact descriptors, which are better stored using solutions that are aware of the semantics involved and can execute server-side operations on the stored data (e.g. a SQL database, or a key-value store).
2. due to the significant size of the artifact contents (e.g. OCI container images, models and datasets), the path they need to take to get between the location where they're stored and the AI/ML workload location where they're consumed or generated should be as short and involve as few intermediary services as possible. Or to formalize: in MLOps, the data path should be used primarily for artifact contents, and the control path should be used for artifact descriptors and other control plane information.
3. one means of facilitating integration between FuseML and external systems is to have FuseML store artifact descriptors that reference artifacts contents stored externally

Having established the distinction between artifact descriptors and artifact contents, the FuseML artifact store components are generally concerned with the following:
1. implements generic operations dealing with artifact descriptors (registering, updating, searching, deleting artifacts)
2. implements persistent storage for artifact descriptors (e.g. in a SQL database or key-value store)
3. registers event hooks and monitors events coming from the storage back-end (e.g. new version of artifact contents published)
4. if needed, the artifact store may interact with the external storage solution used for artifact contents to ensure consistency (e.g. confirm or update versioning, tagging, labeling, metadata where these are supported by the storage solution), delete orphan artifact contents, implement garbage collection etc.

Given FuseML's mission to dynamically integrate with 3rd party AI/ML tools, we'll encounter cases where a 3rd party tool implements its own features related to artifact management, such as storage, tracking, versioning, visualization and so on. In fact, there's a whole category of AI/ML projects with model and/or data versioning listed as their main features (e.g. MLFlow, DVC, Pachyderm, ModelDB, MLRun). When integrating FuseML with such tools, one of the following design patterns can be used to enable FuseML to operate artifacts managed by the 3rd party tool:
1. API adapter - the FuseML artifact store component acts as an API adapter wrapped around the artifact store API exposed by the 3rd party tool. The FuseML artifact store component doesn't store any persistent state related to the 3rd party artifact descriptors. A caching mechanism may be included.
2. API adapter with descriptor storage - this is the API adapter pattern extended to manage its own version of artifact descriptors. The FuseML artifact store stores in its artifact descriptor persistent storage a record of every artifact managed by the 3rd party tool, but doesn't store the artifact contents themselves, it relies on the 3rd party tool to do that. This solution may be required when the 3rd party tool doesn't store enough information or information equivalent to the requirements defined by FuseML regarding artifact descriptors
3. shared back-end storage for artifact contents - FuseML and the 3rd party tool both use the same storage back-end for artifact contents. This is only possible if all the data relevant for FuseML is stored in the back-end by the 3rd party tool, so the 3rd party API can be circumvented.
4. mirror - FuseML keeps a complete copy (contents included) of all artifacts managed by the 3rd party tool. This solution may be particularly useful when artifacts need to be used across kubernetes cluster boundaries and the 3rd party artifact store is only visible inside the cluster.

If FuseML is not the only source of artifacts for a 3rd party store (e.g. 3rd party artifacts can be created by external tools, or by pipeline components directly, without registering them through the FuseML API), then supporting 3rd party artifact stores also needs to be coupled with a mechanism that allows FuseML to be synchronized with the 3rd party tool concerning events that create, modify or delete stored artifacts. This can be done either by ensuring FuseML acts as a proxy for all artifact API operations directed to the 3rd party tool, or implementing a notification and synchronization mechanism that keeps them in sync.

Furthermore, for every flavor of artifact store that FuseML supports, either directly, as a built-in artifact store, or indirectly, through an extension integrating a 3rd party tool, FuseML also needs to facilitate the access path between AI/ML workloads and the artifact contents. This, coupled with the fact that some AI/ML workloads are also managed by 3rd party tools (e.g. for distributed training and prediction serving), seemingly creates a many-to-many adaptation problem that is difficult to solve. In reality, most 3rd party tools dealing with artifact storage provide features that allow artifact contents to be consumed directly from their supported storage backend service. Some notable examples are DVC, Pachyderm and MLFlow, all of which allow translating a dataset or model artifact version into its corresponding location in the back-end storage (e.g. S3 bucket). 

Similarly, most 3rd party tools managing AI/ML workloads are usually integrated with popular object storage services. For every 3rd party artifact store, FuseML only needs to provide the conversion and, optionally, transport of artifacts to and from one of these object storage back-end services. For example, this could take the form of runnables that provide these conversion (and optionally transport) operations, that must be implemented as extensions required for every 3rd party store, that FuseML dynamically and transparently inserts in the pipeline instances depending on the tools that are being used.

For 3rd party artifact stores that don't expose their back-end storage solution, FuseML can employ means of transporting artifacts locally where the AI/ML workloads are located (e.g. into mounted kubernetes volumes), also implemented using 3rd party tool specific runnables.

For visualization and other tool specific features that are not expressed as part of the generic artifact store interface, FuseML should support a redirection mechanism, pointing the user to the 3rd party UI/API. 

Types of artifacts, depending on how they are managed in relation to FuseML:
* internal artifacts: managed by a FuseML artifact store. The FuseML artifact store facilitates, implements or coordinates all operations related to storage, addressing (e.g. versioning, labeling), tracking and consumption (etc. mounting/copying artifacts where workloads can use them as input, providing valid credentials for access etc.). If an external 3rd party artifact store is involved, the FuseML artifact store acts as an intermediary, through the use of tool specific extensions, to hide the implementation specific aspects from the other core services and the machine learning workloads.
* anonymous artifacts: FuseML is not aware of these artifacts. They are either managed directly by 3rd party tools, or being passed directly between pipeline steps (e.g. by means of local temporary storage). 
* external artifacts: artifacts managed externally, by 3rd party tools with which FuseML is otherwise integrated, or by services entirely foreign to FuseML. FuseML may still be able to record external references of these artifacts, but all other operations related to artifact management (e.g. tracking, versioning) are unavailable.

FuseML must define a unified addressing scheme that can be used to identify any artifact regardless of type, tool provenance or storage back-end. It should also provide mechanisms (extensions, adapters, converters, software libraries) to facilitate interoperability in the way AI/ML workloads consume and generate foreign artifacts. For example, a model inference tool should be able to consume a model artifact generated by a training tool even if it doesn't provide built-in compatibility features to support this use-case.

### Codeset Artifact Store

Codeset (formerly _application_) artifacts are organized using generic git concepts: organizations, repositories, branches, tags and commits. A codeset artifact is uniquely identified by a commit number (and the the organization and repository names, without which they don't make sense). 

FuseML ships with a built-in git server (gitea) used to store internal codeset artifacts. 

Integration with external codeset artifacts (e.g. files stored in GitHub or other VCS services) should be supported via 3rd party components that implement the base artifact store interface. External codeset artifacts can also be imported into and regularly synchronized with the internal git server through external means.

### Runnable Artifact Store

The OCI container images associated with FuseML runnables are stored in and managed by FuseML's docker-registry component. References to external OCI container images may also be used, but this cannot be guaranteed to work with FuseML's end-to-end lineage and automation mechanisms.

Q: where does FuseML store the runnable descriptors ? 

### Model Artifact Store

To be detailed.

### Data Artifact Store

To be detailed.

## Pipeline Catalog

## Workload Agents

Runnable artifacts are objects at rest, they embed all the necessary software requirements, configuration files and a description of how they can be executed, but it takes additional information to fully describe the runtime environment and a runtime engine to transform runnables into actual workloads.

In implementing an end-to-end MLOps workflow, FuseML has to orchestrate together types of automated workloads managed by AI/ML tools that are very specific in what they are doing and how they are doing it. Neural architecture search, hyperparameter tuning, distributed training, model inference serving and monitoring are just a few well defined categories of specific AI/ML workloads, and there are many other, more difficult to classify types of workloads and tools listed under AutoML. Runtime agents are services that provide the adaptation layer between the generic AI/ML workload API defined by FuseML and consumed by the core pipeline management services and the more specific 3rd party tools managing those workloads.

Similar to artifact stores, the runtime agent APIs is the demarcation point where the code stops being 3rd party tool agnostic. Runtime agents that are not built-in should be implemented as standalone microservices, using a particular type of runnables.

To simplify lifecycle management for artifact workload agents, the same mechanisms implemented by FuseML and used for MLOps could be reused for this extensibility purpose:
* a special type of runnable, similar to a predictor, is used to run workload agents, exposing a REST or gRPC API that is consumed by the FuseML core services
* FuseML provides all the items needed to simplify defining and implementing new agent runnables: base classes, wrappers, base container images etc.

# Minimal Design

For FuseML to be functional, the following design elements and components are needed at a minimum:
* codesets, runnables and their built-in stores
* pipelines and a pipeline store
* the built-in tekton workload agent and one 3rd party workload agent used for inference serving (e.g. KFServing)

<img src="./img/fuseml-design-minimal.svg">

# Use Cases

## End-To-End Pipeline With MLFlow and KFServing

This section documents a theoretical example of an end-to-end MLOps workflow FuseML use-case involving MLFlow as the tool used for the data preparation and training phases, and KFServing as the tool used for prediction serving. The use-case details the actions triggered by the end users, as well as the flow that the information takes through the various FuseML design components.

The use-case relies on the proper FuseML extensions for MLFlow and KFServing being installed by an actor with FuseML administrative privileges. These are detailed first. Having the extensions available, the other actors (DS - Data Scientist and DE - Data Engineer) can then play different roles in configuring the automated MLOps workflow providing the required workflow inputs and consuming the workflow outputs.  

### MLFlow Extensions

A very basic MLFlow extension for FuseML can be built based on the following assumptions and conventions about what constitutes an MLFlow compatible FuseML codeset:
* _MLProject files_: an `MLProject` file and a `conda.yaml` must be included in the codeset, describing the software dependencies, the entry points and its parameters. This expands on the [MLProject](https://www.mlflow.org/docs/latest/projects.html) conventions enforced by MLFlow on its users.
* _entry points_: there is nothing in the `MLProject` format that can be used to _statically_ identify which entry point plays which role in an MLOps pipeline. In other words, the entry points described in an `MLProject` can do anything from data preparation to model training and validation to things completely unrelated to machine learning. The FuseML MLFlow extension might need to define additional conventions to allow users to identify which entry point should be called for data preparation, which for training, validation, prediction and so on, as well as to describe what type of inputs and outputs the entry points consume and generate (e.g. models, datasets). For simplicity, in this exercise we can assume that every MLFlow compatible codeset defines a default (main) entry point that results in an MLFlow model being trained and stored in the MLFlow tracking service.
* _MLFlow model registry_: the MLFlow code should be written in a way that allows the location and credentials for an MLFlow remote tracking service to be supplied as environment variables, [as specified in the MLFlow docs](https://www.mlflow.org/docs/latest/tracking.html) (e.g. `MLFLOW_TRACKING_URI`, `MLFLOW_S3_ENDPOINT_URL`, `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`). The extension should however allow running MLFlow projects without the need to set up an MLFlow tracking service and should be able to extract models saved locally by the MLFlow code, but for this example, we'll assume an MLFlow tracking service is available
* _artifact tracking_: as previously mentioned, there is no convention enforced by MLFlow that we can use to _statically_ determine whether executing an `MLProject` endpoint will generate artifacts that are of interest to FuseML (e.g. to be consumed by pipelines associated with other 3rd party tools). Similarly, there is no clear way of determining if the entry point generates artifacts _at runtime_, not to mention where those artifacts are stored. Properly solving this problem requires integrating the MLFlow tracking service with FuseML as an external artifact store (at least as a model store). 
* compatibility markers: the codesets containing MLFlow projects need to be explicitly marked by the end user to be recognized as compatible by the MLFlow extension and associated pipelines. Alternatively, we might consider that all codesets containing `MLProject` and `conda.yaml` files are implicitly compatible, but in that case the MLFlow extension logic must be allowed to inspect the files of all applicable codesets (TBD what constitutes an _applicable_ codeset) and maybe even to mark the published codesets somehow as MLFlow compatible (i.e. to avoid having to inspect codeset files every time the pipeline logic needs to determine if a codeset is MLFlow compatible or not).
* MLFlow projects do not lock users into any particular ML library, nor is there anything in `MLProject` files that can be exploited to statically detect which library is used by the underlying code e.g. to save trained models. This makes it difficult to use artifacts generated by MLFlow projects with other tools (e.g. prediction serving tools). The MLFlow extension either needs to automatically detect the ML library used to generate an artifact, or the end user needs to specify that explicitly when publishing the codeset to FuseML. A third solution exists that requires the end user to use a modified MLFlow client library that contains FuseML specific code that is able to communicate all these details to FuseML. For the purpose of this exercise, we'll assume the end user needs to supply this information manually (e.g. as a pipeline parameter).  


Aside from these assumptions and conventions, the MLFlow extension should include the following components:
1. an installation extension that is able to install an MLFlow tracking service
2. a 3rd party tool discovery and registration extension for MLFlow - this is MLFlow specific code that can be invoked by end users to do one of the following:
  * discover existing MLFlow tracking services running in a k8s cluster
  * register an MLFlow tracking services running in a k8s cluster (i.e. store information related to how the tracking service can be accessed: URIs and credentials for the MLFlow API and back-end storage).
  
    For the purpose of this exercise, we can assume this component is not needed and the MLFlow tracking service is always present and can be accessed using the same URIs and credentials, hard-coded somewhere inside the extension code, but this approach is not recommended.

3. running MLFlow projects inside a pipeline requires setting up containers with the proper software requirements described in the `conda.yaml` file provided with the codeset. This can be done as a preparation step lauched before the MLFlow code is executed (note: this approach is already used by seldon core), but installing dependencies is time consuming. To avoid this, a special "MLFlow builder" runnable is required to create container images serving as environments for running the actual MLFlow project entry points. The "MLFlow builder" runnable could be described using the following yaml for example:

  ```
  runnable:
    name: mlflow-builder
    description: |
      Builds a runnable representing a runtime environment that can be used to
      execute the MLFlow code supplied in the input codeset
    kind: builder
    image:
      registryURL: ghcr.io/fuseml
      repository: mlflow-builder
      tag: 1.0
      entrypoint: /build-mlflow-env.py
    inputs:
      - name: mlproject
        type: codeset
        codeset:
          path: /project
          labels:
            - mlflow
            - mlflow-project
    outputs:
      - name: mlflow-env
        type: runnable
        runnable:
          frompath: /project/.fuseml/
    labels:
      - mlflow
      - mlflow-builder
  ```

  The container image should expect the input MLFlow project codeset to be mounted at the given location `/project` and generate a FuseML runnable representing the MLFlow environment container image to be used to run the MLFlow project code available in the input codeset.

  Several FuseML conventions might be used to facilitate generating and registering runnables at runtime from other runnables, some of which are captured here:
  * the builder could save two files at the indicated `/project/.fuseml` path: a `Dockerfile` describing how the image is built and a `runnable.yaml` file containing the runnable descriptor (minus the image reference bits that can be filled in by FuseML). FuseML can implement an additional step in the pipeline that takes in those files, builds the image, saves it in the internal OCI registry, then calls the FuseML API to register the new runnable using the information from the yaml file.
  * alternatively, the builder container could take care of everything: building the image and registering the runnable with the FuseML API (i.e. by using the FuseML client)

  The descriptor for the generated runnable might look like this:

  ```
  runnable:
    name: mlflow-env-<codeset-name>
    description: |
      MLFlow environment to be used for project <codeset-name>
    kind: environment
    image:
      registryURL: fuseml # <- this indicates that the OCI image is stored internally
      repository: mlflow-env-<codeset-name>
      tag:
        - <codeset-name>
        - latest
      entrypoint: mlflow run --no-conda
    inputs:
      - name: mlproject
        type: codeset
        codeset:
          path: /project
          labels:
            - mlflow
            - mlflow-project
        default: "codeset://<codeset-name>/<codeset-version>"
      - name: alpha
        type: float
        default: 5.0
      - name: l1_ratio
        type: float
        default: 0.1
    outputs:
      - name: model-url
        type: string
        fromfile: /project/.fuseml/model-url
    labels:
      - mlflow
      - mlflow-environment
  ```

  Some additional notes and problems that still need to be solved regarding the generated runnable:
  * the `<codeset-name>` and `<codeset-version>` placeholders represent the name and the version of the MLFlow codeset used to generate the runnable
  * an environment runnable doesn't need to be generated every time the MLFlow codeset changes, only when something changes in the `MLProject` or `conda.yaml` files. This means that the same environment runnable version can be reused for several codeset versions. How can this be reflected in the generated runnable descriptor ? For this example, the default value of the input codeset is set to point to the exact codeset version used to generate the environment runnable, which means by default it can _only_ be used with that same codeset.
  * the entrypoint parameters that are specified in the `MLProject` file for the `main` entry point should be reflected as input parameters in the runnable.
  * FuseML must define some convention that can be used by the generated environment runnable to indicate its outputs back to the pipeline logic. In this example, the convention is to save the model's URL inside a file indicated in the `fromfile` attribute, which the FuseML pipeline logic can read and record. This is required if the runnable's output is to be used as input by subsequent pipeline components (e.g. for prediction serving). Ideally, the model should be modeled as an FuseML artifact, same as the generated runnable, e.g.:
    ```
    outputs:
      - name: model-url
        type: model
        model:
          fromfile: /project/.fuseml/model-url
          labels:
            - mlflow
            - sklearn
    ```

  * a builder might generate not one, but several runnables, each corresponding to one entry point configured in the `MLProject` file, all using the same container image


### KFServing Extensions

A KFServing extension for FuseML can be implemented by a simple runnable that takes in a model along with some other prediction parameters and creates a KFServing prediction service for it. Its definition may look like this:

  ```
  runnable:
    name: kfserving-predictor
    description: |
      Creates a KFServing prediction service for the model supplied as input
    kind: predictor
    image:
      registryURL: ghcr.io/fuseml
      repository: kfserving-predictor
      tag: 1.0
    inputs:
      - name: model-url
        type: string
      - name: predictor
        type: string
    outputs:
      - name: prediction-url
        type: string
    labels:
      - kfserving
  ```

### Workflow

The workflow assumes the following:
* FuseML is installed in a kubernetes cluster and the FuseML REST API is exposed and can be accessed from outside the cluster (e.g. from a development workstation)
* 3rd party tools (MLFlow and KFServing) are installed in the same cluster
* extensions detailed in previous paragraphs are also installed (e.g. runnables are registered with FuseML)
* the fuseml CLI is installed where needed

Given all that, running an MLOps pipeline involves the following:

1. defining a pipeline template describing what the automated workflow does and what types of inputs (artifact, parameters) it needs
2. providing all of the pipeline's inputs (artifacts, parameters). An incompletely defined pipeline (a pipeline that doesn't have values for all its inputs) cannot be executed

FuseML should make no assumption about the order in which these steps need to be executed. The pipeline template may be defined before its inputs, in which case it will be automatically triggered when all inputs become available and then subsequently re-triggered whenever new input versions for artifacts are published. Alternatively, all inputs may be available before the pipeline template is created, in which case the pipeline can start immediately upon creation.

The two segments of the workflow are detailed separately in the next sections and they both involve running the fuseml CLI. As a preparation step, the user must configure how the CLI can contact the FuseML API. Several mechanisms could be used for this, all of which should be supported:
* environment variable (e.g. `export FUSEML_URL=http://fuseml.10.20.30.nip.io`)
* command line parameter (e.g. `fuseml --url http://fuseml.10.20.30.nip.io`)
* configuration file (e.g. `~/.fuseml/config`)

Furthermore, when authentication and authorization features are implmented, user credentials will also need to be provided in a similar fashion.

#### Publishing Codesets

In this part of the workflow, the actor responsible for writing MLFlow code (e.g. data scientist) makes the code available to FuseML in the form of a codeset artifact. Several ways of registering FuseML codesets can be imagined, but the simplest one is by running the fuseml CLI on the same machine where the code is located:

* first, create a project - a concept used to group together several codesets and possible other objects, including pipeline templates, with the purpose of applying other operations to the group as a whole (not yet detailed in this document). On the codeset side of things, this is represented by a git organization used as a parent for all codesets belonging to the project. This operation should be restricted so that only FuseML administrators or delegated roles can execute it, but for the purpose of this exercise, we'll assume everyone can do it: 

  ```
  fuseml project create --name my-ml-project --description "My first ML project with FuseML"
  ```

  What happens under the hood:
    * the fuseml CLI calls the remote FuseML REST API to create a project, e.g.;

      POST on http://fuseml.10.20.30.nip.io/api/project with body:
      ```
      project:
        name: my-ml-project
        description: |
          My first ML project with FuseML
      ```

    * the FuseML core service calls the gitea API to create a `my-ml-project` organization (if not yet present). This may be part of the implementation of the "codeset store", although the concept of a project is wider than just codesets and might need to be expanded to cover other core components or even be managed by a separate component (to be determined)
    * if everything works out ok, a success code is returned to the CLI

* next, publish the code in the form of a codeset artifact, indicating the local folder where the code is located, as well as the parent project and the type of codeset (by specifying labels):

  ```
  fuseml codeset push --name my-mlflow-app --description "My first MLFlow application with FuseML" --location local/path/to/code --label mlflow-project --project my-ml-project --branch main
  ```

  What happens under the hood:
    * the fuseml CLI contacts the remote FuseML REST API to check if the project exists:

      GET on http://fuseml.10.20.30.nip.io/api/project?name=my-ml-project with response:
      ```
      project:
        name: my-ml-project
        description: |
          My first ML project with FuseML
      ```

      or:

      GET on http://fuseml.10.20.30.nip.io/api/project/my-ml-project with same response

    * the fuseml CLI then proceeds to check whether a codeset with the same name already exists with e.g.:

      GET on http://fuseml.10.20.30.nip.io/api/codeset?name=my-mlflow-app&project=my-ml-project with response 404

      or:

      GET on http://fuseml.10.20.30.nip.io/api/project/my-ml-project/codeset/my-mlflow-app with response 404

    * then a new codeset is created, e.g.:

      POST on http://fuseml.10.20.30.nip.io/api/codeset with body:
      ```
      codeset:
        name: my-mlflow-app
        description: |
          My first MLFlow application with FuseML
        branch: main
        project: my-ml-project
        labels:
          - mlflow-project
      ```

      or:

      POST on http://fuseml.10.20.30.nip.io/api/project/my-ml-project/codeset with body:
      ```
      codeset:
        name: my-mlflow-app
        description: |
          My first MLFlow application with FuseML
        branch: main
        labels:
          - mlflow-project
      ```

    * the FuseML core service calls the gitea API to create an empty git repository (if not yet present), and set up the proper credentials. The credentials should be returned to the client in the response, alongside the codeset descriptor (depicted here as user/password or auth token), or may need to be retrieved separately, e.g.:

      ```
      codeset:
        uuid: <some unique UUID value>
        created: <date>
        name: my-mlflow-app
        description: |
          My first MLFlow application with FuseML
        branch: main
        project: my-ml-project
        labels:
          - mlflow-project
        version: nil # to indicate that this is an empty repository
        tags: []
        auth:
          username: <user>
          password: <pass>
          token: <token>
      ```

      NOTE: it's not yet clear if all this information is needed or if it can be stored entirely inside gitea, or if FuseML needs an additional persistent state storage back-end (e.g. SQL database) parallel to gitea where to store this information.

    * the FuseML CLI then uses git locally to push the files to the remote repository. There should be some nice integration allowing the end user to operate on a local clone of that repository instead of having to rely on copying files to a /tmp location and sync-ing them from there, but this is outside the scope for this section.

    * finally, if the git push operation succeeds, the CLI _may_ need to contact the FuseML API again to register the new codeset _version_, e.g.:

      PUT on http://fuseml.10.20.30.nip.io/api/project/my-ml-project/codeset/my-mlflow-app with body:
      ```
      codeset:
        version: <git commit number>
        tags: []
      ```

      or, alternatively, just the version:

      POST on http://fuseml.10.20.30.nip.io/api/project/my-ml-project/codeset/my-mlflow-app/version/\<git-commit-number\> with body:
      ```
      version:
        tags: []
      ```

      NOTE: contacting the FuseML API to register new versions is just a design detail that might not be required if FuseML can rely exclusively on gitea provided webhooks. It might be required if/when external codesets are to be supported (i.e. code is stored in external git servers that don't have webhook integration available for FuseML to consume).

    * when notified (either through the REST API or through the back-end gitea webhooks) of the new codeset version, the FuseML core service or its workflow engine (Tekton) must react by triggering all affected pipelines (explored in the pipeline section).

* pushing additional codeset versions will look a bit different. The end user shouldn't need to provide all the information (labels, description) every time a new version is published, unless something needs to change in those attributes. 

  ```
  fuseml codeset push --name my-mlflow-app --location local/path/to/code --project my-ml-project --branch main
  ```

  Better yet, there should be some way to "cache" those attibutes locally (e.g. as files in the code location or under `~/.fuseml`), so the end user may just run:

  ```
  fuseml codeset push --location local/path/to/code
  ```

  What happens under the hood is similar to the previous example, with a few minor changes:
    * the fuseml CLI contacts the remote FuseML REST API to check if the project and codeset already exist (and to get the required credentials needed to push new code to the repository) 
    * next, the new code version is pushed, same as in the previous example
    * finally, if the git push operation succeeds, the CLI _may_ again need to contact the FuseML API to register the new codeset _version_ (see NOTE in the previous example)
    * when notified (either through the REST API or through the back-end gitea webhooks) of the new codeset version, the FuseML core service or its workflow engine (Tekton) must react by re-triggering all affected pipelines (explored in the pipeline section).


    NOTE: fuseml shouldn't exclude the possibility of allowing end users to run git commands directly to push new codeset versions, without involving the fuseml CLI at all. In fact, just based on the workflows detailed here, there is nothing preventing users from doing that directly, as long as they have the correct credentials.

* additional operations available for codesets, similar to those used internally by the pipeline logic to validate how codeset artifacts can be used as inputs for pipelines, should in theory also be available through the CLI, e.g.:

  ```
  # list all MLFlow compatible codesets (only most recent versions) in the project
  fuseml codeset get --label mlflow-project --project my-ml-project
  ```

  What happens under the hood:
    * the fuseml CLI contacts the remote FuseML REST API to run a codeset query:

      GET on http://fuseml.10.20.30.nip.io/api/codeset?label=mlflow-project&project=my-ml-project 

      GET on http://fuseml.10.20.30.nip.io/api/project/my-ml-project/codeset?label=mlflow-project

    * the FuseML core service implements the request by means of the "codeset store" component, which in its simplest form can be just a transparent adapter that calls the gitea API to retrieve the information. In this example, it would have to access the gitea API to search for all repositories in the `my-ml-project` organization that have the associated `mlflow-project` label.

#### Configuring Pipelines

This part of the workflow involves the actor responsible for configuring the GitOps/MLOps automation pipelines (e.g. data engineer or devops enginner), although it's not excluded that the data scientist may have some limited involvment in this, or it can even be the same actor. 

A real ML application may require not one but several pipelines and codesets being acted on by several actors to be managed together as a single MLOps workflow. Furthermore, it may be preferable that some or all of the MLOps workflow layout not only resemble, but be actually built on top of or out of the same workflows that data scientists are using during the experimentation phase, which is highly suggestive of a collaborative effort involving all these actors in building the final MLOps workflow for an ML application.

The example described in this section is minimal. It assumes that in the experimentation phase, the data scientist is creating a monolith: a single logical block of code (represented by a single MLFlow codeset) that bundles together all the steps that make up the experimentation part of the pipeline required to train a machine learning model: data preparation, model definition, training and validation. However, theoretically, the FuseML design and its pipelines are flexible enough to allow data scientists to split these monoliths into modules that can be further orchestrated as individual, composable and reusable pipeline components.

The following high-level composable pipeline segment templates are featured in this exercise:
* builder segment: takes in an MLFlow compatible codeset artifact as input and generates a runnable artifact representing the environment that can be used to run any of its entry points. This is based on the `mlflow-builder` runnable installed by the MLFlow extension
* trainer segment: takes in an MLFlow compatible codeset artifact as input, runs it and outputs the resulted trained ML model. This segment is based on the runnable generated at runtime by the builder segment
* predictor segment: takes in a trained ML model as input, starts a prediction service for it and outputs its exposed endpoint (i.e. the URL where the service receives HTTP calls for prediction). This is based on the `kfserving-predictor` runnable installed by the KFServing extension

Each of these segments can be instantiated and executed independently, as long as there are available inputs (e.g. compatible codesets and models) that they can consume and as long as the values of all other input parameters are explicitly specified (or have default values). The steps detailed in the next part will cover defining the individual pipeline templates and then building a master end-to-end pipeline composed of all three segments which is automatically run according to the available MLFlow compatible codeset versions published as detailed in the _Publishing Codesets_ section. 

##### Inline Pipeline Templates

Before going into how runnables can be used as building blocks for pipelines, here's what the complete end-to-end pipeline may look like _without_ the use of runnables as composable components:

  ```
  pipeline-template:
    name: mlflow-kfserving-e2e
    description: |
      End-to-end pipeline template that takes in an MLFlow compatible codeset,
      runs the MLFlow project to train a model, then creates a KFServing prediction 
      service that can be used to run predictions against the model.
    inputs:
      - name: mlflow-codeset
        description: an MLFlow compatible codeset
        type: codeset
        labels:
          - mlflow-project
      - name: predictor
        description: type of predictor engine
        type: string
        default: auto
    outputs:
      - name: prediction-url
        description: the URL where the exposed prediction service endpoint can
        be contacted to run predictions 
        type: string
    steps:
      - name: builder
        image:
          registryURL: ghcr.io/fuseml
          repository: mlflow-builder
          tag: 1.0
        inputs:
          - name: mlflow-codeset
            codeset:
              path: /project
        outputs:
          - name: mlflow-env
            image:
              fromfile: /project/.fuseml/Dockerfile
              repository: "mlflow-builder/{{inputs[mlflow-codeset].name}}"
              tag: "{{inputs[mlflow-codeset].version}}"
      - name: trainer
        image:
          registryURL: fuseml # <- this indicates that the OCI image is stored internally
          repository: "mlflow-builder/{{inputs[mlflow-codeset].name}}"
          tag: "{{inputs[mlflow-codeset].version}}"
        inputs:
          - name: mlflow-codeset
            codeset:
              path: /project
        outputs:
          - name: mlflow-model-url
            fromfile: /project/.fuseml/model-url
      - name: predictor
        image:
          registryURL: ghcr.io/fuseml
          repository: kfserving-predictor
          tag: 1.0
        inputs:
          - name: model
            value: "{{steps[trainer].outputs[mlflow-model-url]}}"
          - name: predictor
            value: "{{inputs[predictor]}}"
        outputs:
          - name: prediction-url
            fromfile: /project/.fuseml/prediction-url
  ```

Some notes worth mentioning:
* the inputs and outputs of pipeline templates are listed globally, and then subsequently referenced (by name) in the steps where they are used. This also allows the pipeline templates themselves to be reused as composable elements, eligible to be referenced as parts of other pipeline templates, in a recursive manner.
* the pipeline needs to support expressions such as `"mlflow-builder/{{inputs[mlflow-codeset].name}}"` which are expanded either at definition time or at runtime. The alternative is to hard-code those values in the template, which reduces or even altogether elliminates the value of the template.
* the first and second steps, `builder` and `trainer`, both accept a codeset as input. FuseML expands the pipeline will the necessary job steps and resources with required to clone and mount the codeset where these job steps can use it. This way, the pipeline creator doesn't have to deal with accessing the codesets.
* the first step, `builder` has an output of type `image`. This is supplied by the step's container at runtime as a Dockerfile. FuseML expands the pipeline to follow up with a FuseML specific job step that takes in that Dockerfile, builds the image and saves it in the internal OCI registry. This way, the pipeline creator doesn't have to deal with container building and storing and the `mlflow-builder` image can be kept simple instead of having to deal with the complexities of building container images (inside a container) and registering them with the internal registry

Some issues that remain to be solved:
* the location of and credentials needed for the MLFlow tracking service need to be passed on to the `trainer` step as environment variables. This information needs to come from somewhere, most likely some form of registry for 3rd party tool instances to which FuseML is connected at runtime. A similar situation applies to the `predictor` step, that needs a kubernetes service account to interact with the KFServing API to create the prediction service.
* the type of predictor needed for the `predictor` step depends on the library used by the MLFlow project to train the model. It can simply be `mlflow`, as some predictors (e.g. Seldon Core) features an mlflow predictor. However, for other types of predictors (e.g. `sklearn`, `pytorch`, `tensorflow`), there is no simple way of detecting which ML library is used to save an MLFlow model. The end user might have to indicate it explicitly by using an additional codeset label that needs to be passed to the predictor step as a parameter value. 

The workflow initiated by the actor responsible for managing pipeline templates may look as follows:
* first, create a project, or use an existing project. This is just a convention used to group and organize FuseML objects together. The details are already covered in the _Publishing Codesets_ section, so it's only briefly mentioned here.

  ```
  fuseml project create --name my-ml-project --description "My first ML project with FuseML"
  ```

* next, publish the pipeline template definition described earlier.

  ```
  fuseml pipeline create --project my-ml-project -f e2e-pipeline.yaml
  ```

  What happens under the hood:
    * the fuseml CLI calls the remote FuseML REST API to create a pipeline template, e.g.:

      POST on http://fuseml.10.20.30.nip.io/api/pipeline with the pipeline yaml file contents as the body

    * the FuseML core service passes the pipeline template description to the component responsible for implementing pipelines (i.e. the Tekton agent), where the pipeline template is converted into corresponding Tekton objects.
    * note that the input codeset parameter value isn't fully defined. The FuseML core service may also perform a search, looking for all compatible codesets already published in the same project and automatically set up webhooks and even trigger pipeline runs corresponding to each compatible codeset.
    * if everything works out ok, a success code is returned to the CLI

* there should be several ways supported by FuseML in which a pipeline template can be instantiated:

  1. one or more pipeline templates could be configured by the MLOps engineer to be automatically instantiated based on the availability of codesets in the same project. In our case, this means that whenever an MLFlow compatible codeset version is published anywhere in the `my-ml-project` project, a pipeline instance taking in that codeset is automatically instantiated and executed from the e2e pipeline template.
  2. the MLOps engineer could configure one or several pipeline templates but not set them to run automatically. This is also the case when not all the values of input parameters can be inferred from codeset artifacts (or any other artifacts for that matter). Such pipeline templates need to be instantiated explicitly via the FuseML CLI. In our case, assuming that the `predictor` input parameter didn't have a default value, the e2e pipeline couldn't be instantiated automatically when a codeset is published. The actor publishing codesets (data engineer) would need to manually trigger the pipeline and provide the `predictor` value, e.g.:

      ```
      # list all pipeline templates configured for this project that are compatible with my codeset:
      fuseml pipeline list --project my-ml-project --codeset my-mlflow-app
      fuseml pipeline run mlflow-e2e --input predictor=sklearn --input mlflow-codeset=my-mlflow-app
      ```

      or even better, if the project and codeset are assumed to be part of the implicit CLI context:

      ```
      # list all pipeline templates compatible with my implicit codeset
      fuseml pipeline list
      # run the e2e pipeline with the explicit predictor value and the implicit codeset 
      fuseml pipeline run mlflow-e2e --input predictor=sklearn
      ```

      or we could even consider a "publish-and-run" CLI shortcut:

      ```
      fuseml codeset push --location local/path/to/code --run mlflow-e2e --input predictor=sklearn
      ```

##### Composable Pipeline Templates

Runnables are the basic composable building blocks out of which more elaborated pipeline templates can be created. Creating pipeline templates out of existing runnables or other pipeline templates could be described as follows:
* list the components: runnables and/or other pipeline templates that are part of the template. The order in which the components need to be executed may be indicated explicitly by specifying dependency parameters. There is also an implicit order that can be inferred from the way inputs and outputs are connected 
* optionally, list the pipeline template global inputs and outputs. These can be directly mapped to the inputs and outputs of participating components, or their values can be set to expressions referencing those inputs and outputs
* specify values for some or all of the component inputs. The input of a component can be set to an explicit value, or directly mapped to the output of another component or of a global template input, or its value can be set to a expression referencing global inputs and other outputs. Ultimately, component inputs and outputs may be left "unconnected", in which case they will either represent global inputs and outputs, or FuseML may be instructed to automatically attempt to connect compatible inputs with compatible outputs

The end to end pipeline built out of runnables published by participating extensions could look like this: 

  ```
  pipeline-template:
    name: mlflow-kfserving-e2e
    description: |
      End-to-end pipeline template that takes in an MLFlow compatible codeset,
      runs the MLFlow project to train a model, then creates a KFServing prediction 
      service that can be used to run predictions against the model.
    inputs:
      - name: mlflow-codeset
      - name: predictor
    outputs:
      - name: prediction-url
    steps:
      - name: builder
        runnable: mlflow-builder
        inputs:
          - name: mlproject
            inputref: mlflow-codeset
      - name: trainer
        runnable: "{{steps[builder].outputs[mlflow-env]}}"
        inputs:
          - name: mlproject
            inputref: mlflow-codeset
        after: builder
      - name: predictor
        runnable: kfserving-predictor
        inputs:
          - name: model
            value: "{{steps[trainer].outputs[model-url]}}"
          - name: predictor
            inputref: predictor
        outputs:
          - name: prediction-url
            outputref: prediction-url
  ```

Alternatively, if FuseML supports automatically matching inputs and outputs of components, it can be trimmed down to:

  ```
  pipeline-template:
    name: mlflow-kfserving-e2e
    description: |
      End-to-end pipeline template that takes in an MLFlow compatible codeset,
      runs the MLFlow project to train a model, then creates a KFServing prediction 
      service that can be used to run predictions against the model.
    steps:
      - name: builder
        runnable: mlflow-builder
      - name: trainer
        runnable: "{{steps[builder].outputs[mlflow-env]}}"
        after: builder
      - name: predictor
        runnable: kfserving-predictor
        inputs:
          - name: model
            value: "{{steps[trainer].outputs[model-url]}}"
  ```


Some observations:
* global inputs and outputs don't need to be fully specified (e.g. to include the type, description etc.), if they are mapped to the inputs and outputs of components
* the second step, `trainer` may need to be explicitly marked to be executed after `builder`, because its definition depends on its outcome

# Questions

- What is the actual expected workflow from user perspective? Are described artefacts visible (and editable) by end user or are there more of an internal concepts? It seems potentially useful to allow users to edit (exchange/plug) various components of the whole workflow, I worry a little bit that it would become overcomplicated.

**A**: the actual workflow can take a variety of forms, involving many actors, from people who only want to be concerned with writing ML code (data scientists), to people who want to customize everything from runnables and pipelines and all the way to writing their own extensions to integrate with additional 3rd party tools. The usual workflow would require FuseML admins to install "extensions" available in some form of marketplace, where an extension bundles together runnables, pipeline templates and workload agents. Regular FuseML users would only use these extensions and pipelines through the CLI and/or UI, so they won't be exposed to the implementation details.

For example, let's say we have a FuseML instance running with the following extensions installed by the admin:
* "MLFlow" providing a runnable and pipeline template for training ML models with MLFlow
* "KFServing" providing a runnable and pipeline template for serving ML models with KFServing

The workflow could be started by an MLOps engineer preparing a FuseML project, consisting of a git organization and a set of attached pipeline templates:

```
fuseml project create myproj
# used to set myproj as the current project instead of supplying it explicitly to all other commands
fuseml project set myproj  
fuseml pipeline add <mlflow-training-pipeline-template-name> --name training-pipeline --input mlflow_code='code://*'
fuseml pipeline add <kfserving-pipeline-template-name> --name serving-pipeline --input model='model://*'
```
If there's an end-to-end pipeline template available with sane defaults, this could even be shortened to:

```
fuseml codeset push path/to/code --name myapp
fuseml pipeline add <end-to-end-pipeline-template-name> --name full-pipeline --input mlflow_code='code://*'
```

By specifying wildcards for inputs, the pipelines are configured to run automatically for any codesets and models registered within the project.

The Data Scientists can simply publish their MLFlow code to FuseML. Something like this, for example:

```
# used to set myproj as the current project instead of supplying it explicitly to all other commands
fuseml project set myproj 
fuseml codeset push path/to/code --name myapp
```

FuseML will automatically run all pipelines configured by the MLOps engineer, making sure to maintain lineage according to versioning data, so the results that were produced out of the "current" code version can be shown with e.g.:

```
fuseml pipeline results show --current-code --recursive
Pipeline/Run                     Inputs                                             Outputs
------------                     ------                                             -------
training-pipeline/1              mlflow_code: code://myproj/myapp/f3b0924           model: model://myproj/4c88e8f252be0a8e1e8e3
serving-pipeline/1               model: model://myproj/4c88e8f252be0a8e1e8e3        url: http://myapp.myproj.10.20.30.40.nip.io/predict
                                 predictor: sklearn (default)                       
```

The FuseML CLI can even be made to accept a --wait flag to make it wait until all pipelines directly and indirectly related to the current change have finished running, while at the same time printing information describing the pipeline steps being executed.

The roles may also vary: it can be the DS who is doing everything, from project creation to code publishing.

- I think ideally, user should only provide the input (i.e. codeset) and an action what to do (train/serve/etc.) with various optional arguments. But even for such a simple case (like the MLflow example), the resulting pipeline consists of several, sometimes non-trivial steps. I assume we would need to provide library (store) of such steps (built on top of runnables) that could be used to build whole pipelines. We should allow users to define (register) their own runnables, but it should not be a requirement.

**A**: FuseML needs to provide some "built-in" runnables or steps for common pipeline operations. For example, for handling inputs and outputs (copying/mounting/registering artifacts). We could make FuseML aware of more specific AI/ML concepts like "train" and "serve", which would make it a lot easier for end users to define their workflows, but we should not be exclusive about the type of pipelines that users can run, to ensure extensibility.

- Artefacts like Codeset, Runnable and Pipeline seems to be quite independent in theory and possibly not relying on specific implementation. This seems good at a first glance. However, look how the carrier based fuseml is done - gitea setup provides webhooks which are hooked to Tekton through listener service. Do you expect we build our own listener, actually making fuseml the service that executes the hooks? That might be nice again from the architecture POV, it seems unnecessary to reinvent the wheel again.

**A**: webhooks are required when FuseML isn't explicitly notified about new artifact versions being created. For example, if someone runs `git push` instead of `fuseml push` inside a git repository representing a FuseML codeset, webhooks are needed to notify FuseML about the new codeset version, because `fuseml push` also calls the FuseML API to register the new codeset version. Yes, we can rely on the direct integration between Tekton and gitea through webhooks to trigger pipelines, but the core FuseML service would have no direct visibility into what goes on with the artifacts... unless Tekton communicates this information back to FuseML ?. Tekton webhook integration will be enough for an alpha version. We'll just have to wait and see if we need to build our own webhook listener at some point, especially since the same argument might apply to other types of artifacts (runnables, models, datasets).