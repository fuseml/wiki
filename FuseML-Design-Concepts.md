**WIP ---- WIP ---- WIP ---- WIP**

# Basic Design Concepts

## Artifacts

Artifacts can be generally described as immutable objects at rest that are either generated by or accepted as input by AI/ML workloads orchestrated by FuseML. Some example of artifacts are: ML models (can be just model definitions or full blown pre-trained models), container images built for various ML operations (data transformation, training, inference serving etc.), ML code components or packages and datasets.

Artifacts can be stored and referenced through a variety of mechanisms (versioning, tagging, labeling etc.), but their contents is static. The immutability aspect is vital for features like gitops operations, end-to-end lineage and reproducibility. This abstraction should also extend past code and container images and cover the full range of objects involved in AI/ML operations, including but not limited to models and datasets. Even so, achieving true reproducibility will not be possible, since random factors cannot be completely eliminated from the AI/ML workloads themselves. For example, resuming a failed MLOps pipeline or running a single pipeline component will generate a new set of artifacts rather than replace the artifacts already generated by the previous runs, but this can be managed by manipulating the references (versions, tags, labels) such that they point to the new generation of artifacts instead of the previous one.

Artifacts play another important role in constructing MLOps pipelines: they facilitate composability and reusability. Artifacts act as connection points that enable building a pipeline out of reusable components. Two or more pipeline components can be coupled together to form higher level operational concepts by listing in their definition the type of artifacts that they consume and generate. When artifacts are well described, building complex functional pipelines out of components becomes a simple exercise of matching inputs to outputs.

FuseML must be able to track all artifacts used or generated by AI/ML workloads and implement other conventions around artifacts that are needed to implement additional features important from an MLOps workflow perspective. For example:
* searching for artifacts using compatibility filters (e.g. ML models compatible with TensorFlow, predictors compatible with Seldon Core)
* navigating the history of artifacts generated by subsequent runs of the pipeline or runnable that generated them or used them as input

### Application artifacts

An application in its most basic form is represented by a collection of files: code files, scripts, configuration files, generally all the sources needed to implement, build and execute the machine learning logic that goes into the automated ML operations orchestrated by FuseML. It often happens with sample AI/ML applications that datasets and even ML models are grouped and stored alongside regular application files, but this practice is not advisable, because datasets and ML models represent different categories of AI/ML artifacts, with their own specific lifecycle and operations and FuseML expects to be able to process them differently then it does application files.  

An application artifact is an immutable snaphot of an application. FuseML relies on the internal git server to store application files and organize them as immutable snapshots that can be versioned, tagged, branched etc. From this standpoint, an internal application artifact is represented by a commit in a git repository.

### Runnable artifacts

FuseML runnables are the composable, reusable and shareable building blocks used for ML operations. A runnable is represented by an OCI container image paired with one or more FuseML specific descriptors that describe how the container image can be used as a component in the MLOps workflow. Runnables are also one of the most important extensibility mechanisms employed by FuseML to integrate with 3rd party AI/ML tools in a loosely coupled manner.

The complete format of the FuseML runnable descriptor is outside the scope of this document, but some suggestions of fields it should include are given here:
* runnable type - to differentiate between several sub-classes of runnables, each with its own specifics regarding how they are organized and operated on by FuseML
* entry point (command, binary or script to execute)
* list of named and typed inputs and outputs. Some possible options are:
  * artifacts or artifact references (e.g. datasets, models, other runnables), optionally accompanied by additional specifiers describing the artifact type, provenance, version etc. Specifiers can be used for example to further filter the artifacts that match the inputs compatible with the runnable, make suggestions about which runnables can be coupled, detect incorrect pipeline setups etc.
  * for input artifacts in particular, the descriptor might include additional details describing statically what constitutes a change (e.g. which files to watch for in an input application artifact, which fields to watch for in an input data artifact and which metrics to watch for in an input model artifact)
  * parameters passed by value (e.g. strings, integers, booleans). These can be modeled to take several forms at runtime: 
    * environment variables
    * command line arguments
    * config maps
* resource requirements (ram, cpu, gpu, storage)
* capability requirements (e.g. cpu type, gpu type)

FuseML must automatically handle all aspects needed to provide the artifacts used as input by the runnable's container, as well as collect and register the artifacts generated by the container as outputs, in order to isolate the builder's code from the FuseML specific aspects of storing and versioning the artifacts (e.g. clone or copy the git repo/commit, model or dataset into a volume where the container has access).

Examples of subclasses of runnable artifacts modeled as core FuseML concepts that are covered in the next sections:
* builders - used to build other runnables out of application artifacts 
* trainers - used to train a machine learning model
* predictors - used to run predictions on a pre-trained model

More classes of runnables will be modeled as they are identified as common patterns that need to be represented as core FuseML concepts, as we integrate additional 3rd party AI/ML tools with FuseML, for example:
* data transformers - used for data ETL and paired with a feature store
* model validators - used to validate a pre-trained model against test or new data 
* model explainers

#### Builders

Builders are runnables that build other runnables out of application artifacts. Builders can themselves be built out of code using other builders and so on, in a recurring manner. FuseML should provide some default builder runnables and conventions that can in turn be used to create other builders specifically designed for 3rd party tools.

What is specific about these runnables:
  * inputs include one (or more) application artifact(s)
    * sources to be built into runnables
    * scripts used to build the runnables
    * configuration files controlling how to build the runnables
  * outputs include one or more runnable artifacts
  * other types of artifacts should be allowed as inputs and outputs. For example:
    * a builder might take in an application artifact that represents application code bundled together with datasets and models. Aside from a trainer artifact, this builder may create additional dataset artifacts and model artifacts corresponding to those bundled models and datasets.
    * a builder might take in another runnable artifact e.g. if it needs to extend its logic
  * FuseML must facilitate the creation and registration of runnable artifacts by providing one or more of the following
    * CLI
    * SDK libraries
    * container images bundling together the CLI/SDK with other common tools required to define runnables and their software requirements. These container images can be used directly or as base images for tool specific images.
    * automation for building container images: the builder should not be concerned with _building_ container images. It suffices to provide Dockerfiles describing these images and let FuseML take care of all aspects of building, storing, tagging etc. 

Builder runnables can be executed using the builtin FuseML pipeline engine (Tekton). 

Examples of builder artifacts:
1. MLFlow builder - builds trainer and predictor runnables out of an MLFlow project
   * ID: MLFlow project builder
   * inputs:
     * an application artifact of type "MLFlow project", with a change filter configured to trigger only when the `conda.yaml` or `MLProject` files change
     * (implicit) an application artifact of type "MLFlow builder" - contains the scripts required to build MLFlow runnable artifacts
   * outputs:
     * a trainer artifact of type "MLFLow" (described below)
     * a predictor artifact of type "MLFLow" (described below)
   * OCI container:
     * stock image (or derivation of) provided by FuseML for all builder runnable artifacts, supplying base software requirements (FuseML CLI or SDK library)
   * entry point: the custom builder logic provided with the "MLFlow builder" application artifact that leverages the `MLProject` and `conda.yaml` files bundled within the MLFlow application artifact:
     * determines if changes from the previous input artifact versions warrant a rebuild of trainer/predictor artifacts (e.g. did the `conda.yaml` file change ? or the builder logic itself ? or was the builder triggered manually or scheduled to run ?) 
     * creates a Dockerfile for the container image used in common by trainer and predictor (installs mlflow and all software requirements specified in `conda.yaml`)
     * uses FuseML CLI/SDK to register the Dockerfile and the descriptors for the two runnables with FuseML
       * the runnables will be tied to the same input application artifact of type "MLFlow project" as one used to run the builder
       * the parameters for the training runnable are inferred from the `MLProject` file

   Usage example: whenever a new MLFlow project application version is published, the builder is triggered and registers new versions of the "MLFlow" trainer and predictor runnables. FuseML continues the process by rebuilding the container image before recording the new runnables in its database and triggering other workflows that have been affected.

NOTE: the current MLFlow builder is implemented using several Tekton tasks instead of a single container.



#### Trainers

Trainers are runnables that can be used to train a machine learning model. 

What is specific about these runnables:
  * inputs include:
    * one or more dataset artifact(s) (or zero, if the dataset is included in the container image, in another artifact or obtained through other means that don't involve FuseML)
    * training parameters (can be used to do hyperparameter tuning) 
  * outputs include:
    * one or more model artifacts
    * metrics
  * other types of artifacts should be allowed as inputs and outputs. For example:
    * a trainer might take in application artifacts, for one of these reasons:
      * that's where the dataset or model definition are bundled
      * because the application artifact contains other files required for training (e.g. parameters)
      * the training code itself is bundled there (i.e. code doesn't need to be built or packaged, python for example)
    * the trainer may generate one or more datasets (e.g. the processed dataset used for training, the one used for validation) to be used for later reference (e.g. to detect when drifts occur)
  * FuseML must facilitate the creation and registration of model artifacts by providing one or more of the following
    * CLI
    * SDK libraries (e.g. python libraries extending popular machine learning libraries and providing means of storing trained models)

Trainer runnables can be executed using different engines implemented by different tools:
* the default: FuseML builtin engine (Tekton)
* other engines depend on the model training orchestration tools that are integrated with FuseML. The same trainer runnable could in theory be executed using various engines used for distributed training, hyper-parameter tuning etc.


Examples of trainer artifacts:
1. MLFlow trainer - trains a model out of a specific MLFlow project
   * ID: MLFlow trainer for project X
   * inputs:
     * an application artifact of type "MLFlow project" (pinned down to point to the specific application artifact branch as the one used to build the trainer)
     * environment variables pointing to the MLFlow instance where experiments are logged (to be determined if and how these are provided)
     * (optional) URL to data
     * training parameters (extracted from the `MLProject` file)
   * outputs:
     * a model of type "MLFLow" (or a model of type "Y" where Y is the ML library used to train and save the model - e.g. sklearn, pytorch, tensorflow)
   * OCI container:
     * custom container image build by the "MLFlow builder" runnable and including all software requirements characteristic to the project
   * entry point: script wrapping `mlflow run`:
     * runs the project (assumes it does training among other things)
     * detects the saved model from the output
     * uses FuseML CLI/SDK to register the model with FuseML
     * alternatively, the MLFlow project code files themselves use the SDK to register the model with FuseML, or FuseML is already integrated with MLFlow by other means and can detect when a model is registered

   Usage example: whenever a new version of the target MLFlow project application is published or when a new trainer version is published, the trainer is triggered to train and register a new version of the model. The trainer can also be triggered to run manually, with different training parameter or with different data.

### Predictors

# Higher Level Design Concepts

## Artifact Store

Storing artifacts outside the context of the MLOps pipeline that generates them (i.e. in a centralized artifact store) opens up additional possibilities. If pipeline components can consume inputs from and save outputs to an artifact store, they no longer have to be tightly coupled together into complete pipelines in order to be functional. Pipeline components become standalone functions that can be executed individually. Running a pipeline step-by-step in a controlled manner (e.g. for debugging purposes) is also easily achievable, as well as dynamically swapping components.

It is not mandatory that all steps in a pipeline register their output artifacts with an artifact store through FuseML. It may make more sense to keep artifacts anonymous, when for example artifacts are not relevant, or when it is not efficient to sync artifacts with a central store due to size concerns. Artifacts can be passed between pipeline components by using local, temporary storage. Lineage and reproducibility features will not be available, but only isolated to the pipeline steps that don't follow this convention. Interoperability (different pipeline steps implemented by different tools) is still achievable with anonymous artifacts, if they are well defined, even if they are not stored, but this still creates additional dependencies between pipeline steps regarding local artifact storage.

Given FuseML's mission to dynamically integrate with 3rd party AI/ML tools, we'll encounter cases where a 3rd party tool implements its own features related to artifact management, such as storage, tracking, versioning, visualization and so on. In fact, there's a whole category of AI/ML projects with model and/or data versioning listed as their main features (e.g. MLFlow, DVC, Pachyderm, ModelDB, MLRun). When integrating FuseML with such tools, one of the following modes of operations can be used to ensure that the FuseML API can be used to operate artifacts stored by the 3rd party tool:
1. transparent proxy - FuseML acts as an API proxy that can be used to access the artifact store API exposed by the 3rd party tool
2. catalog proxy - FuseML keeps in its database a record of every artifact managed by the 3rd party tool, but doesn't store the artifacts themselves, it relies on the 3rd party tool to do that
3. shared back-end storage - FuseML and the 3rd party tool both use the same storage back-end. This is only possible if all the data relevant for FuseML is stored in the back-end by the 3rd party tool, so the 3rd party API can be circumvented.
4. mirror - FuseML keeps a copy of all artifacts managed by the 3rd party tool. This is particularly important when artifacts need to be used across kubernetes cluster boundaries and the 3rd party store is only visible inside the cluster. 

Supporting 3rd party artifact stores also needs to be coupled with a mechanism that allows FuseML to be synchronized with the 3rd party tool concerning events that create, modify or delete stored artifacts. This can be done either by ensuring FuseML acts as a proxy for all artifact API operations directed to the 3rd party tool, or implementing a notification and synchronization mechanism that keeps them in sync. For visualization and other tool specific features, FuseML should support a redirection mechanism, pointing the user to the 3rd party UI/API. 

Types of artifacts, depending on how they are managed in relation to FuseML:
* internal artifacts: managed internally by FuseML. FuseML directly facilitates, implements or coordinates all operations related to storage, addressing (e.g. versioning, labeling), tracking and consumption (etc. mounting/copying artifacts where workloads can use them as input, providing valid credentials for access etc.). If an external 3rd party artifact store is involved, FuseML acts as an intermediary, through the use of tool specific extensions, to hide the implementation specific aspects from the machine learning workloads.
* anonymous artifacts: FuseML is not aware of these artifacts. They are either managed remotely by 3rd party tools, or locally, by being passed directly between pipeline steps by means of local temporary storage. 
* external artifacts: FuseML may still be able to keep records of artifacts even when they are managed externally, by 3rd party tools with which FuseML is otherwise integrated, or by services entirely foreign to FuseML. External artifacts will not benefit from FuseML's features like monitoring and tracking.  

FuseML must define a unified addressing scheme that can be used to identify any artifact regardless of type, tool provenance or storage back-end. It should also provide mechanisms (extensions, adapters, converters, software libraries) to facilitate interoperability in the way AI/ML workloads consume and generate foreign artifacts. For example, a model inference tool should be able to consume a model artifact generated by a training tool even if it doesn't provide built-in compatibility features to support this use-case.

### Application Artifact Store

Internal application artifacts are stored and managed exclusively via the Fuseml git server component. External application artifacts (e.g. application files stored in GitHub or other VCS services) are not supported directly at the moment, because FuseML is tightly integrated with the git server component, but external application artifacts can easily be imported into and regularly synchronized with the internal git server through external means.

### Runnable Artifact Store

The OCI container images associated with FuseML runnables are stored in and managed by FuseML's docker-registry component. References to external OCI container images may also be used, but this cannot be guaranteed to work with FuseML's end-to-end lineage and automation mechanisms.

Q: where does FuseML store the runnable descriptors ? 

### Model Artifact Store

Not planned ATM.

### Data Artifact Store

Not planned ATM.

